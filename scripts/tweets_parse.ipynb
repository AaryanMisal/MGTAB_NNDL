{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25764ad4",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ab0b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AdityaNangia/Desktop/ADITYA/A College/COLUMBIA/Sem 2/NNDL/Project/MGTAB_NNDL/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch, json, glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85f88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a71c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b73164bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b303ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8061620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3de2e",
   "metadata": {},
   "source": [
    "### Model -> LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b0f911e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "model = model.to(torch.float16)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c43f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_files = glob.glob('../Dataset/TwiBot-22/tweet_*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a930779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures to accumulate\n",
    "sum_embeds   = defaultdict(lambda: torch.zeros(768, device=device))\n",
    "tweet_counts = defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf07e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../Dataset/TwiBot-22\"\n",
    "\n",
    "DATA_DIR = Path(DATA_DIR)\n",
    "# make sure the data directory exists\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Data directory {DATA_DIR} does not exist.\")\n",
    "\n",
    "def load_json_records(fname):\n",
    "    \"\"\"Load a JSON file of array- or line- delimited records.\"\"\"\n",
    "    path = DATA_DIR / fname\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        # if the file is a single large JSON array:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            # fallback: one JSON object per line\n",
    "            f.seek(0)\n",
    "            data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "user_dicts = load_json_records('user.json')\n",
    "users_df = pd.DataFrame(user_dicts)\n",
    "\n",
    "ordered_uids = users_df['id'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24305972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Hyperparameters & Files ─────────────────────────────────────────────────\n",
    "BATCH_SIZE           = 512\n",
    "CHECKPOINT_INTERVAL  = 1_000_000        # save every 1M tweets\n",
    "CHECKPOINT_FILE      = 'tweet_feats_checkpoint.pkl'\n",
    "TWEET_GLOB_PATTERN   = '../Dataset/TwiBot-22/tweet_*.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe9715",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3a9be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../Dataset/TwiBot-22/tweet_1.json\n",
      "Batch size: 512\n",
      "mps\n",
      "Encoded 1000 tweets in 1.77s → 563.6 tweets/sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "sample_texts = []\n",
    "for fn in glob.glob('../Dataset/TwiBot-22/tweet_*.json'):\n",
    "    print(f\"Loading {fn}\")\n",
    "    with open(tweet_files[0], 'r') as f:\n",
    "        for tw in ijson.items(f, 'item'):\n",
    "            text = tw.get('text','').strip()\n",
    "            if text:\n",
    "                sample_texts.append(text)\n",
    "            if len(sample_texts) >= 1000:\n",
    "                break\n",
    "    break\n",
    "\n",
    "# 2) Time the batch encode\n",
    "t0 = time.time()\n",
    "_  = model.encode(sample_texts,\n",
    "                  convert_to_tensor=True,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  show_progress_bar=False)\n",
    "t1 = time.time()\n",
    "\n",
    "throughput = len(sample_texts) / (t1 - t0)\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(device)\n",
    "print(f\"Encoded {len(sample_texts)} tweets in {t1-t0:.2f}s → {throughput:.1f} tweets/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e616e9",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7dce1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    with open(CHECKPOINT_FILE, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        sum_embeds, tweet_counts, processed = (data['sum_embeds'],\n",
    "                                               data['tweet_counts'],\n",
    "                                               data['processed'])\n",
    "        sum_embeds = sum_embeds.to(device)\n",
    "    print(f\"Resuming from checkpoint: {processed} tweets processed so far.\")\n",
    "else:\n",
    "    sum_embeds   = defaultdict(lambda: torch.zeros(768, device=device))\n",
    "    tweet_counts = defaultdict(int)\n",
    "    processed    = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44c6e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_uids, batch_texts = [], []\n",
    "def flush_batch():\n",
    "    \"\"\"Encode batch_texts, accumulate into sum_embeds/tweet_counts,\n",
    "       advance processed counter, and checkpoint if needed.\"\"\"\n",
    "    global processed\n",
    "    if not batch_texts:\n",
    "        return\n",
    "\n",
    "    # 2a) Batch-encode\n",
    "    embs = model.encode(\n",
    "        batch_texts,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    # 2b) Accumulate\n",
    "    for uid, emb in zip(batch_uids, embs):\n",
    "        sum_embeds[uid]   += emb\n",
    "        tweet_counts[uid] += 1\n",
    "\n",
    "    # 2c) Update processed count & clear buffers\n",
    "    processed += len(batch_texts)\n",
    "    batch_uids.clear()\n",
    "    batch_texts.clear()\n",
    "\n",
    "    # 2d) Checkpoint?\n",
    "    if processed % CHECKPOINT_INTERVAL < BATCH_SIZE:\n",
    "        with open(CHECKPOINT_FILE, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'sum_embeds':   sum_embeds,\n",
    "                'tweet_counts': tweet_counts,\n",
    "                'processed':    processed\n",
    "            }, f)\n",
    "        print(f\"Checkpoint saved at {processed} tweets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba7f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   0%|          | 0/9 [04:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m batch_texts\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_texts) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m BATCH_SIZE:\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mflush_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m, in \u001b[0;36mflush_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2a) Batch-encode\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m embs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 2b) Accumulate\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m uid, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_uids, embs):\n",
      "File \u001b[0;32m~/Desktop/ADITYA/A College/COLUMBIA/Sem 2/NNDL/Project/MGTAB_NNDL/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:681\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    673\u001b[0m             features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    674\u001b[0m                 (\n\u001b[1;32m    675\u001b[0m                     features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    679\u001b[0m             )\n\u001b[0;32m--> 681\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Desktop/ADITYA/A College/COLUMBIA/Sem 2/NNDL/Project/MGTAB_NNDL/.venv/lib/python3.10/site-packages/sentence_transformers/util.py:1209\u001b[0m, in \u001b[0;36mbatch_to_device\u001b[0;34m(batch, target_device)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], Tensor):\n\u001b[0;32m-> 1209\u001b[0m         batch[key] \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ─── 3) Stream & process with tqdm ───────────────────────────────────────────\n",
    "tweet_files = glob.glob(TWEET_GLOB_PATTERN)\n",
    "for fn in tweet_files:\n",
    "    with open(fn, 'r') as f:\n",
    "        # ijson.items streams each JSON object in the top-level array\n",
    "        for tw in tqdm(ijson.items(f, 'item'),\n",
    "                       desc=f\"Streaming {os.path.basename(fn)}\",\n",
    "                       leave=False):\n",
    "            text = tw.get('text','').strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            batch_uids.append(tw['author_id'])\n",
    "            batch_texts.append(text)\n",
    "\n",
    "            if len(batch_texts) >= BATCH_SIZE:\n",
    "                flush_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d816f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_batch()\n",
    "with open(CHECKPOINT_FILE, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'sum_embeds':   sum_embeds,\n",
    "        'tweet_counts': tweet_counts,\n",
    "        'processed':    processed\n",
    "    }, f)\n",
    "print(f\"✅ Done! Total tweets processed: {processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673165e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet‐feature tensor size: torch.Size([1000000, 768])\n"
     ]
    }
   ],
   "source": [
    "user_tweet_feats = []\n",
    "for uid in ordered_uids:\n",
    "    cnt = tweet_counts.get(uid, 0)\n",
    "    if cnt > 0:\n",
    "        avg = sum_embeds[uid] / cnt\n",
    "    else:\n",
    "        avg = torch.zeros(768)     # no tweets → zero vector\n",
    "    user_tweet_feats.append(avg)\n",
    "\n",
    "# shape [num_users, 768]\n",
    "tweets_tensor = torch.stack(user_tweet_feats, dim=0)\n",
    "\n",
    "# sanity check\n",
    "print(\"Tweet‐feature tensor size:\", tweets_tensor.shape)\n",
    "# should be (len(ordered_uids), 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e1bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tweets_tensor, 'tweets_tensor.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
